{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f59aea",
   "metadata": {},
   "source": [
    "### 1. What is Simple Linear Regression?\n",
    "\n",
    "Simple Linear Regression is a statistical method where we predict one dependent variable (Y) using one independent variable (X). It tries to find the best straight-line relationship between X and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ffc551",
   "metadata": {},
   "source": [
    "### 2. What are the key assumptions of Simple Linear Regression?\n",
    "\n",
    "- The relationship between X and Y is linear.\n",
    "- Residuals (errors) are normally distributed.\n",
    "- Homoscedasticity: constant variance of residuals.\n",
    "- No or minimal multicollinearity.\n",
    "- Residuals are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e5d0f",
   "metadata": {},
   "source": [
    "### 3. What does the coefficient m represent in the equation Y=mX+c?\n",
    "\n",
    "The coefficient 'm' is the **slope** of the line. It tells how much Y changes when X increases by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ef2b0",
   "metadata": {},
   "source": [
    "### 4. What does the intercept c represent in the equation Y=mX+c?\n",
    "\n",
    "'c' is the **intercept**. It's the value of Y when X is 0. Basically, it's where the line cuts the Y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99adfc",
   "metadata": {},
   "source": [
    "### 5. How do we calculate the slope m in Simple Linear Regression?\n",
    "\n",
    "m = (mean(XY) - mean(X)*mean(Y)) / (mean(X²) - mean(X)²)\n",
    "It shows how Y changes with respect to X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aaa9c9",
   "metadata": {},
   "source": [
    "### 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
    "\n",
    "Least squares finds the best-fit line by minimizing the **sum of squared residuals** (difference between actual and predicted Y values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a56cea",
   "metadata": {},
   "source": [
    "### 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
    "\n",
    "R² shows how much of the variation in Y is explained by X. If R² = 0.85, it means 85% of the variation in Y is explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe3cc6",
   "metadata": {},
   "source": [
    "### 8. What is Multiple Linear Regression?\n",
    "\n",
    "Multiple Linear Regression predicts a dependent variable using **two or more independent variables**. It extends simple linear regression to handle more predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7eb9c",
   "metadata": {},
   "source": [
    "### 9. What is the main difference between Simple and Multiple Linear Regression?\n",
    "\n",
    "Simple Linear Regression uses **one** independent variable.\n",
    "Multiple Linear Regression uses **two or more** independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718f4d0",
   "metadata": {},
   "source": [
    "### 10. What are the key assumptions of Multiple Linear Regression?\n",
    "\n",
    "- Linearity\n",
    "- No multicollinearity\n",
    "- Homoscedasticity\n",
    "- Independence of residuals\n",
    "- Normally distributed residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caceb0da",
   "metadata": {},
   "source": [
    "### 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "\n",
    "Heteroscedasticity means that the variance of the residuals (errors) is not constant across all levels of the independent variables. It can lead to unreliable p-values and confidence intervals, making our model less trustworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115c9fb",
   "metadata": {},
   "source": [
    "### 12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
    "\n",
    "To deal with multicollinearity:\n",
    "- Remove or combine highly correlated variables.\n",
    "- Use techniques like Principal Component Analysis (PCA).\n",
    "- Use Ridge or Lasso Regression to reduce the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c542ff6",
   "metadata": {},
   "source": [
    "### 13. What are some common techniques for transforming categorical variables for use in regression models?\n",
    "\n",
    "We usually use:\n",
    "- One-Hot Encoding\n",
    "- Label Encoding\n",
    "- Ordinal Encoding\n",
    "These let regression models understand non-numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72bb00",
   "metadata": {},
   "source": [
    "### 14. What is the role of interaction terms in Multiple Linear Regression?\n",
    "\n",
    "Interaction terms show how two variables together affect the outcome. They help model situations where the effect of one variable depends on another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3038eb",
   "metadata": {},
   "source": [
    "### 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "\n",
    "In Simple Linear Regression, the intercept is the value of Y when X = 0.\n",
    "In Multiple Linear Regression, it’s the value of Y when all independent variables = 0. But sometimes, this value doesn’t make real-world sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05c95d",
   "metadata": {},
   "source": [
    "### 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "\n",
    "The slope shows the effect of each independent variable on the dependent one. A higher or lower slope tells us how sensitive Y is to changes in X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a50e15",
   "metadata": {},
   "source": [
    "### 17. How does the intercept in a regression model provide context for the relationship between variables?\n",
    "\n",
    "The intercept gives a starting point for predictions — the value of Y when all Xs are zero. It gives context to how the rest of the variables affect the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ccd79",
   "metadata": {},
   "source": [
    "### 18. What are the limitations of using R² as a sole measure of model performance?\n",
    "\n",
    "R² doesn’t tell us:\n",
    "- If the model is the best fit.\n",
    "- If the predictors are significant.\n",
    "- How it performs on new data.\n",
    "Also, it always increases with more variables, even if they’re not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea81ceb",
   "metadata": {},
   "source": [
    "### 19. How would you interpret a large standard error for a regression coefficient?\n",
    "\n",
    "A large standard error means there's more uncertainty in that coefficient. It might not be statistically significant, and its real effect could be weak or unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c185f9",
   "metadata": {},
   "source": [
    "### 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "\n",
    "In residual plots, heteroscedasticity looks like a funnel shape — the spread of residuals increases or decreases. It’s important to fix it because it messes up confidence intervals and hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764773d",
   "metadata": {},
   "source": [
    "### 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
    "\n",
    "It means the model might have too many unnecessary variables. R² increases with more variables, even if they aren’t useful. Adjusted R² corrects for this, so a low adjusted R² shows that many predictors might not actually improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903be8c2",
   "metadata": {},
   "source": [
    "### 22. Why is it important to scale variables in Multiple Linear Regression?\n",
    "\n",
    "Scaling makes sure that all variables are on the same scale. This is important when:\n",
    "- We’re comparing coefficients.\n",
    "- Using regularization (like Lasso or Ridge).\n",
    "It avoids giving more importance to variables with larger values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b8988",
   "metadata": {},
   "source": [
    "### 23. What is polynomial regression?\n",
    "\n",
    "Polynomial regression is a type of regression where the relationship between X and Y is modeled as an nth-degree polynomial instead of a straight line. It fits curves to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f167128",
   "metadata": {},
   "source": [
    "### 24. How does polynomial regression differ from linear regression?\n",
    "\n",
    "In linear regression, we fit a straight line.\n",
    "In polynomial regression, we fit a curved line using powers of X (like X², X³, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d54ec",
   "metadata": {},
   "source": [
    "### 25. When is polynomial regression used?\n",
    "\n",
    "It’s used when the data shows a non-linear relationship. If the line doesn’t fit well and there's a curve, polynomial regression gives better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457116f3",
   "metadata": {},
   "source": [
    "### 26. What is the general equation for polynomial regression?\n",
    "\n",
    "Y = b₀ + b₁X + b₂X² + b₃X³ + ... + bₙXⁿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296541f",
   "metadata": {},
   "source": [
    "### 27. Can polynomial regression be applied to multiple variables?\n",
    "\n",
    "Yes, we can apply it to multiple variables — that’s called Multivariate Polynomial Regression. Each variable can have higher-order powers and interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf33a5",
   "metadata": {},
   "source": [
    "### 28. What are the limitations of polynomial regression?\n",
    "\n",
    "- It can easily overfit if degree is too high.\n",
    "- It’s sensitive to outliers.\n",
    "- Doesn’t work well outside the range of training data (poor extrapolation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d6260",
   "metadata": {},
   "source": [
    "### 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "\n",
    "We can use:\n",
    "- Adjusted R²\n",
    "- Cross-validation scores\n",
    "- AIC/BIC values\n",
    "- Train-test split performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcda29e",
   "metadata": {},
   "source": [
    "### 30. Why is visualization important in polynomial regression?\n",
    "\n",
    "Visualization helps us see the curve and check if it fits the data well. It makes it easier to understand whether the model is underfitting, overfitting, or just right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93b6b0",
   "metadata": {},
   "source": [
    "### 31. How is polynomial regression implemented in Python?\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}